---
layout:     post   				    # 使用的布局（不需要改）
title:      人工智能-神经网络			    # 标题 
subtitle:                 #副标题
date:       2020-02-04	# 时间
author:     Btbsja					# 作者
header-img: img/bg-2020-02-03.jpg 	    #这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签

    - 人工智能

---

# 神经网络

## 一、实验目的

理解反向传播网络的结构和原理，掌握反向传播算法对神经元的训练过程，了解反向传播公式。通过构建BP网络实例，熟悉前馈网络的原理及结构。

## 二、实验内容

1.  通过BP网络各项参数的不同设置，观察BP算法的学习效果。观察比较BP网络 拓朴结构及其它各项参数变化对于训练结果的影响。观察并分析不同训练数据对相同拓朴结构的BP网络建模的影响。

2.  设计简单的感知器，实现简单的逻辑运算（与、或）等，也可做其他更复杂的问题。

## 三、实验环境

1.  神经网络可视化实验环境，如图3所示。

> ![](https://gitee.com/btbsja/BlogImg/raw/master/blog/2020/03/20200309104505.png){width="4.388888888888889in" height="3.0416666666666665in"}

图3神经网络可视化实验环境

2\. C＋＋语言编程环境，语言环境可以自选。

## 四、实验步骤

> 1.进入神经网络可视化实验环境，基本实验步骤如下：
>
> (1)进入实验环境；
>
> (2)选择相关的实验模块；
>
> (3)设置相应地实验参数（如设置初始权值为随机值）；
>
> (4)选择实验运行方式；
>
> (5)观测运行过程；
>
> (6)修改相应地参数重复第1\~5步直到满意为止。

**对设计型实验可参照以下步骤：**

> (1)进入实验环境；

(2)进入创建新模型工作窗；

(3)创建网络拓朴结构；

(4)设置相应的网络参数；

(5)输入相应的训练数据集；

(6)设置实验环境参数；

(7)选择相应的运行方式；

(8)观察实验过程

(9)修改相关参数并重复1\~8步直至满意为止。

2.编写简单的感知器学习程序，训练感知器执行操作。

## 五、实验结论

包括做实验的目的、方法、过程等，具体要写成实验报告, 见后附表三。

1、BP网络的基本结构及BP算法的训练过程。

2、试述阈值函数和权值变化对BP网络推理结果的影响。

3、训练数据集变化对网络训练结果的影响。

## 附：神经网络实验报告表三

| 实验目的         | 理解反向传播网络的结构和原理，掌握反向传播算法对神经元的训练过程，了解反向传播公式。通过构建BP网络实例，熟悉前馈网络的原理及结构。 |                                                              |                                                              |
| ---------------- | :----------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 网络  拓朴图     | ![image-20200309104226123](https://gitee.com/btbsja/BlogImg/raw/master/blog/2020/03/20200309104506.png) | ![image-20200309104222498](https://gitee.com/btbsja/BlogImg/raw/master/blog/2020/03/20200309104507.png) | ![image-20200309104236451](https://gitee.com/btbsja/BlogImg/raw/master/blog/2020/03/20200309104508.png) |
| 训练  数据集     | （输入节点 0，输入节点 1，输入节点 2，输出节点 5）  （0,0,0,0 ）（0,0,1,0 ）（0,1,1,1 ） （1,0,0,0 ）（1,0,1,1 ）（1,1,0,1 ） （1,1,1,1 ） | （输入节点 0，输入节点 1，输出节点 4） （0,0,0 ）（0,1,0 ）（1,0,1 ） | （Known，New，Short ，Home,Reads）   （1,1,0,1,0）  （0,1,1,0,1）（0,0,0,0,0 ）（1,0,0,1,0 ） （1,1,1,1,1 ）（1,0,0,0,0 ） （0,0,1,0,0 ）（0,1,1,0,1 ） （1,0,0,1,0 ）（1,1,0,0,0 ） （0,0,1,1,0 ）（1,1,0,0,0 ） （1,0,1,1,1 ）（1,1,1,0,1 ） （1,1,1,1,1 ）（1,0,1,0,1 ） （1,1,1,1,1 ）（0,1,1,0,1 ） |
| 训练误差         | 第 1 代误差   0.039   第 51 代误差   0.02   第 101 代误差   0.02   第 151 代误差   0.01   第 201 代误差   0.01 | 第 1 代误差   0.018   第 51 代误差   0.010   第 101 代误差   0.010   第 151 代误差   0.010   第 201 代误差   0.010 | 第 1 代误差   4.67 第 51代误差 0.66 第 101 代误差 0.12 第 151 代误差   0.06 第 201 代误差 0.03 |
| 模拟的问题或函数 | 多数赞成表决器                                               | 异或问题                                                     | MailReading （邮件信息识别）                                 |
| 观测结果         | 经过 100多代的进化，误差以明显的阶梯型降低  ![image-20200309104311348](https://gitee.com/btbsja/BlogImg/raw/master/blog/2020/03/20200309104509.png) | 由于初始误差比较低，  故经过 50代的进化，误差已经极大地降低，几乎不再变化  ![image-20200309104319052](https://gitee.com/btbsja/BlogImg/raw/master/blog/2020/03/20200309104510.png) | 经过200 代的进化，误差极大地降低  ![image-20200309104326185](https://gitee.com/btbsja/BlogImg/raw/master/blog/2020/03/20200309104511.png) |
| 学生结论         | 神经计算能够实现“多数赞成表决器”功能                         | 单层的神经网络无法实现异或问题，但是含有中间层的BP网 络却可以很好的解决异或问题 | 经过训练的BP网络可以进行邮件识别，解决信息识别的难题，可以极大地提高生产力 |

## 感知器实现

![](https://gitee.com/btbsja/BlogImg/raw/master/blog/2020/03/20200309104512.png){width="6.166666666666667in" height="4.636111111111111in"}



## 源代码

```
#include <iostream>
#include <stdlib.h>
 #include <math.h>
using namespace std;
const unsigned int nTests   =4; //训练数据的数量
const unsigned int nInputs  =2; //输入端的数量
const double alpha =0.3;   //学习参数
double weight1[nInputs]={0.0};
double weight2[nInputs]={0.0};
double b=-0.8;      //阈值设为-0.8
struct slp
{
    double inputs[nInputs];
    double output;
};

//计算输出值
double compute(double *inputs,double * weights,double b)
{
    double sum =0.0;
    for (int i = 0 ; i < nInputs; ++i)
    {
    sum += weights[i]*inputs[i];
    }
    //bias
    sum +=b;
    return sum;
}
int main(){
    /*
 *正确的“与”操作，“或”操作的训练数据，也就是所谓的“target”
 */
slp yu[nTests] = {
    {0.0,0.0,0.0},
    {0.0,1.0,0.0},
    {1.0,0.0,0.0},
    {1.0,1.0,1.0}
};
slp huo[nTests] = {
    {0.0,0.0,0.0},
    {0.0,1.0,1.0},
    {1.0,0.0,1.0},
    {1.0,1.0,1.0}
};
    bool bLearningOK = false;
    int count1=0;
  //感知器“与”操作学习算法
    while(!bLearningOK)
    {
    bLearningOK = true;
    for (int i = 0 ; i < 4 ; i++)
    {
    double output = compute(yu[i].inputs,weight1,b);
    while(output>0)
    {
        weight1[0]=weight1[0]+alpha*(yu[i].output-output)*yu[i].inputs[0];
        weight1[1]=weight1[1]+alpha*(yu[i].output-output)*yu[i].inputs[1];
        output = compute(yu[i].inputs,weight1,b);
        count1 ++;
        if(count1 >500){
        cout<<"请重设参数"<<endl;
        exit(0);
        }
    }
    if(i==3){
        while(output<0){
        weight1[0]=weight1[0]+alpha*(yu[i].output-output)*yu[i].inputs[0];
        weight1[1]=weight1[1]+alpha*(yu[i].output-output)*yu[i].inputs[1];
        output = compute(yu[i].inputs,weight1,b);
        count1 ++;
        if(count1 >500){
        cout<<"请重设参数"<<endl;
        exit(0);
        }
        }
    }
    }
    }
    bLearningOK = false;
    int count2=0;
  //感知器“或”操作学习算法
    while(!bLearningOK)
    {
    bLearningOK = true;
    //第一种情况要求小于0
    double output = compute(huo[0].inputs,weight2,b);
    while(output>=0){
        //cout<<output;
        weight2[0]=weight2[0]+alpha*(huo[0].output-output)*huo[0].inputs[0];
        weight2[1]=weight2[1]+alpha*(huo[0].output-output)*huo[0].inputs[1];
        output = compute(huo[0].inputs,weight2,b);
        count2 ++;
        if(count2 >500){
        cout<<"请重设参数"<<endl;
        exit(0);
        }
        }
    for (int i = 1 ; i < 4 ; i++)
    {
    double output = compute(huo[i].inputs,weight2,b);
    while(output<0)
    {
        weight2[0]=weight2[0]+alpha*(huo[i].output-output)*huo[i].inputs[0];
        weight2[1]=weight2[1]+alpha*(huo[i].output-output)*huo[i].inputs[1];
        output = compute(huo[i].inputs,weight2,b);
        count2 ++;
        if(count2 >500){
        cout<<"请重设参数"<<endl;
        exit(0);
        }
    }

    }
    }
    cout<<"学习数据："<<endl;
     for(int w = 0 ; w < nInputs  ; ++w)
    {
    cout<<"yu_weight"<<w<<":"<<weight1[w] <<endl;
    }
    cout<<"\n";
    for (int i = 0 ;i < nTests ; ++i)
    {
    cout<<"yu_rightresult："<<yu[i].output<<"\t";
    cout<<"yu_caculateresult:" << compute(yu[i].inputs,weight1,b)<<endl;
    }

    for(int w = 0 ; w < nInputs  ; ++w)
    {
    cout<<"huo_weight"<<w<<":"<<weight2[w] <<endl;
    }
    cout<<"\n";
    for (int i = 0 ;i < nTests ; ++i)
    {
    cout<<"huo_rightresult："<<huo[i].output<<"\t";
    cout<<"huo_caculateresult:" << compute(huo[i].inputs,weight2,b)<<endl;
    }
    cout<<endl<<"学习完毕，请输入两个值："<<endl;
    double test[2];
    cin>>test[0]>>test[1];
    double yu_result;double huo_result;
    if(compute(test,weight1,b)>0)
    yu_result=1;
    else
    yu_result=0;
    if(compute(test,weight2,b)>0)
    huo_result=1;
    else
    huo_result=0;
    cout<<"与操作结果:"<<yu_result<<endl;
    cout<<"或操作结果:"<<huo_result<<endl;
    return 0;
}

```