---
layout:     post   				    # 使用的布局（不需要改）
title:      文本分类		    # 标题 
subtitle:                 #副标题
date:       2020-01-21			# 时间
author:     Btbsja					# 作者
header-img: img/bg-2020-01-20.jpg 	    #这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签

    - 信息内容安全

---

# 文本分类

## 一、实验内容

1、 数据源sogou新闻数据集，题目一和题目二任选一个。

2、 对语料库进行数据预处理，利用朴素贝叶斯算法或SVM完成对测试集的文本进行分类

3、 对语料库进行数据预处理，利用不同特征提取方法，如chi2(卡方)、MI等，利用贝叶斯，KNN，SVM等分类算法实现文本分类，并对比不同特征数下的分 类的准确率

4、 比较不同方法下分类的准确率（选做）

## 二、 实验环境

实验环境：Python3.6.2，

IDE为Pycharm 2019.1.3

## 三、 实验功能及代码

### （一）数据预处理

利用jieba，对原始文本进行分词和去停用词。代码如下：

\# -\*- coding: utf-8 -\*-

import jieba

def word\_segment(inpath, outpath):

stopwords = {}

fso = open(\'C:/Users/lulu/Desktop/题目一/stopwords.txt\'.decode(\'utf-8\'), \'rb\')

for word in fso:

stopwords\[word.strip().decode(\'utf-8\', \'ignore\')\] = word.strip().decode(\'utf-8\', \'ignore\')\#将停用词保存为字典，查询速度快

fso.close()

fin = open(inpath).readlines()

fout = open(outpath, \'wb\')

\#jieba.cut\_for\_search(4)

for line in fin:

lines = line.strip().decode(\'utf-8\', \'ignore\')

word\_list = list(jieba.cut(lines))

result = \'\'

for word in word\_list:

if word not in stopwords:\#去停用词

if word != \'contenttitle\' and word != \'content\' and word != \'docs\' and word != \'doc\':

result += word

result += \' \'

result\_word=result.strip().encode(\'utf-8\')

fout.writelines(result\_word)

fout.close()

\#对每一篇预处理

def preprocess\_text(inRootPath,outRootPath):

category = os.listdir(inRootPath)

for categoryName in category:

categoryPath = os.path.join(inRootPath,categoryName)

out\_categoryPath = os.path.join(outRootPath,categoryName)

filesList = os.listdir(categoryPath)

for filename in filesList:

word\_segment(categoryPath+\'/\'+filename, out\_categoryPath+\'/\'+filename)

\#调用函数preprocess\_text

preprocess\_text(\'C:/Users/lulu/Desktop/题目一/题目三数据集/\'.decode(\'utf-8\'),\'C:/Users/lulu/Desktop/题目一/题目三数据集分词后/\'.decode(\'utf-8\'))

### （二）读入经过分词等预处理后的文件，划分训练集和测试集

代码如下：

def read\_text(rootPath):

contents = \[\]

labels = \[\]

category = os.listdir(rootPath)

for categoryName in category:

categoryPath = rootPath + \'/\' + categoryName

\# categoryPath = os.path.join(rootPath,categoryName) \# 这个类别的路径

filesList = os.listdir(categoryPath) \# 这个类别内所有文件列表

for filename in filesList:

\# lines=open(os.path.join(categoryPath,filename))

lines = open(categoryPath + \'/\' + filename).readlines()

for i in range(len(lines)):

if len(lines\[i\]) \> 200:

contents.append(lines\[i\].decode(\'utf-8\'))

labels.append(categoryName)

return contents, labels, lines

rootPath = unicode(\'D:/PyCharm Community Edition 2018.1/项目/result/\',\'utf-8\')

docs, labels, lines = read\_text(rootPath)

from sklearn.model\_selection import train\_test\_split

X\_train0, X\_test0, y\_train, y\_test = train\_test\_split(docs, labels, test\_size=0.2)

### （三）基于tfidf进行特征提取，使用贝叶斯进行分类

代码如下：

from sklearn.feature\_extraction.text import TfidfVectorizer

\#将文档列表转换为词向量模型scipy.sparse.csr.csr\_matrix

vectorizer = TfidfVectorizer(stop\_words=None, sublinear\_tf=True, max\_df=0.5)

X\_train = vectorizer.fit\_transform(X\_train0)

\# print len(X\_train),len(X\_train\[0\])

X\_test=vectorizer.transform(X\_test0)

\#Bayes

from sklearn.naive\_bayes import MultinomialNB \# 导入多项式贝叶斯算法

clf = MultinomialNB(alpha=0.001).fit(X\_train, y\_train)

y\_pridict=clf.predict(X\_test)

\#评估

from sklearn.metrics import classification\_report

print classification\_report(y\_test,y\_pridict)

### （四）基于词频、chi2进行特征提取，使用贝叶斯进行分类

代码如下



from sklearn.feature\_extraction.text import CountVectorizer

\#将文档列表转换为词向量模型scipy.sparse.csr.csr\_matrix

vectorizer = CountVectorizer(binary=True)

X\_train = vectorizer.fit\_transform(X\_train0)

X\_test=vectorizer.transform(X\_test0)

\#卡方校验提取k维特征

from sklearn.feature\_selection import SelectKBest,chi2

ch2=SelectKBest(chi2,k=7000)\#\#\#

X\_train=ch2.fit\_transform(X\_train,y\_train).toarray()

X\_test=ch2.transform(X\_test).toarray()

\#Bayes

from sklearn.naive\_bayes import MultinomialNB \# 导入多项式贝叶斯算法

clf = MultinomialNB(alpha=0.001).fit(X\_train, y\_train)

y\_pridict=clf.predict(X\_test)

\#评估

from sklearn.metrics import classification\_report

print classification\_report(y\_test,y\_pridict)

## 四、实验截图

![](https://gitee.com/btbsja/BlogImg/raw/master/blog/2020/03/20200309090537.jpeg){width="4.743055555555555in" height="3.658929352580927in"}

## 五、实验总结

1.从运行结果看来，tfidf和chi2两种特征提取方法对分类效果的影响相差不大。其中，采用tfidf提取的特征，最后的分类效果稍微好一点。

2.使用贝叶斯进行文本分类的准确率和召回率较高，均达96%。